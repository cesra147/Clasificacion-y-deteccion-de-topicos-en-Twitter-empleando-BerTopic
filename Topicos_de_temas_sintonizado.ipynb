{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from bertopic import BERTopic\n",
    "import demoji\n",
    "import spacy\n",
    "import unidecode\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stop_words = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\CARG\\Desktop\\Codigos_tesis_bertopic\\medio_año_2022_politica.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtro por mes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_source'] = pd.to_datetime(df['date_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba = df[df['date_source'].dt.month.isin([6])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filas = df_prueba.shape[0]\n",
    "\n",
    "# Imprimir el número de filas\n",
    "print(\"El dataframe df_prueba tiene\", num_filas, \"filas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminacion de emojis\n",
    "def clean_text(x):\n",
    "  x = str(x)\n",
    "  x = x.lower()\n",
    "  x = re.sub(r'#[A-Za-z0-9]*', ' ', x)\n",
    "  x = re.sub(r'https*://.*', ' ', x)\n",
    "  \n",
    "\n",
    "  x = re.sub(r'@[A-Za-z0-9]+', ' ', x)\n",
    "  \n",
    "  # remove emojis\n",
    "  demoji.download_codes()\n",
    "  x = demoji.replace(x, '')\n",
    "\n",
    "  # remove accents\n",
    "  x = unidecode.unidecode(x)\n",
    "  \n",
    "  tokens = word_tokenize(x)\n",
    "  x = ' '.join([w for w in tokens if not w.lower() in stop_words])\n",
    "  x = re.sub(r'[%s]' % re.escape('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~“…”’'), ' ', x)\n",
    "  x = re.sub(r'\\d+', ' ', x)\n",
    "  x = re.sub(r'\\n+', ' ', x)\n",
    "  x = re.sub(r'\\s{2,}', ' ', x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba['clean_text'] = df_prueba.text.apply(clean_text)\n",
    "df_prueba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematización de la columna 'clean_text' del dataframe df_prueba\n",
    "df_prueba['lemmatized_text'] = df_prueba['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = df_prueba.date_source.to_list()\n",
    "tweets = df_prueba.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "umap_model = UMAP(n_neighbors=20, n_components=5, min_dist=0.0, metric='cosine',random_state = 42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=spanish_stop_words)\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "topic_model = BERTopic(\n",
    "        verbose=False,        \n",
    "        embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        language=\"Spanish\",\n",
    "        umap_model=umap_model,              # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words\n",
    "        nr_topics=\"auto\",                        # Step 6 - Diversify topic words\n",
    "        n_gram_range=(1, 3),\n",
    "        calculate_probabilities=False        \n",
    "    )\n",
    "    \n",
    "topics, probs = topic_model.fit_transform(tweets)\n",
    "filtered_text = df_prueba['lemmatized_text']\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": filtered_text,\n",
    "                          \"ID\": range(len(filtered_text)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "df = pd.DataFrame(topic_info, columns=['Tema', 'Palabras clave', 'Frecuencia'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv(\"resultados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = embedding_model\n",
    "docs = tweets\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "# Train BERTopic\n",
    "#topic_model = BERTopic().fit(docs, embeddings)\n",
    "\n",
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "\n",
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=5, min_dist=0.1, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(docs, topics, strategy=\"c-tf-idf\")\n",
    "topic_model.update_topics(docs, topics=new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, \n",
    "                                hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv(\"resultados_reducidos.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(tweets, timestamps, \n",
    "                                                global_tuning=True, evolution_tuning=True, nr_bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
